import json
import logging
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
import networkx as nx
from datetime import datetime
import matplotlib.pyplot as plt
import os

# ğŸ›  ××©×ª× ×™× ×‘×¡×™×¡×™×™×
LOG_FILE = "data_enrichment.log"
EXPORT_DIR = "enriched_data_reports"

# ğŸ¯ ×™×¦×™×¨×ª ×ª×™×§×™×™×ª ×“×•×—×•×ª
if not os.path.exists(EXPORT_DIR):
    os.makedirs(EXPORT_DIR)

# ğŸªµ ×œ×•×’×™×
logging.basicConfig(filename=LOG_FILE, level=logging.INFO, format="%(asctime)s - %(message)s")

# ğŸ“¥ ×©×œ×™×¤×ª × ×ª×•× ×™× ×œ×§×œ×˜
def fetch_data_for_enrichment():
    """×˜×•×¢×Ÿ × ×ª×•× ×™× ××§×•×‘×¥ enriched_sample_data.json ×‘××§×•× ×-API"""
    logging.info("ğŸ“ ×˜×•×¢×Ÿ × ×ª×•× ×™× ××ª×•×š enriched_sample_data.json...")
    try:
        with open("enriched_sample_data.json", "r", encoding="utf-8") as f:
            return json.load(f)
    except Exception as e:
        logging.error(f"×©×’×™××” ×‘×§×¨×™××ª enriched_sample_data.json: {e}")
        return []

# ğŸ“¡ ×¡×™××•×œ×¦×™×” ×©×œ enrichment (××™×Ÿ ×§×¨×™××” ×—×™×¦×•× ×™×ª)
def enrich_data(data):
    """××—×–×™×¨ ××ª ×”× ×ª×•× ×™× ×›×¤×™ ×©×”×, ×œ×œ× ×”×¢×©×¨×” ×—×™×¦×•× ×™×ª"""
    logging.info("ğŸ”„ ××™×Ÿ ×¦×•×¨×š ×‘×”×¢×©×¨×” â€“ ×˜×•×¢×Ÿ × ×ª×•× ×™× ×›××•×ª ×©×”×.")
    return data

# ğŸ“Š × ×™×ª×•×— ×§×œ××¡×˜×¨×™× ×•×’×¨×£ ×§×©×¨×™×
def perform_analysis_on_enriched_data(enriched_data):
    """× ×™×ª×•×— ×”×§×©×¨×™× ×•-clusters"""
    logging.info("ğŸ“Š ××‘×¦×¢ × ×™×ª×•×—×™× ××ª×§×“××™× ×¢×œ ×”× ×ª×•× ×™×...")

    df = pd.DataFrame(enriched_data)

    if 'feature' in df.columns:
        features = df[['feature']].values
        scaler = StandardScaler()
        scaled = scaler.fit_transform(features)
        kmeans = KMeans(n_clusters=3, n_init="auto")
        df['cluster'] = kmeans.fit_predict(scaled)
    else:
        df['cluster'] = -1  # fallback if no features

    G = nx.Graph()
    for _, row in df.iterrows():
        G.add_edge(row['entity_id'], row['related_entity_id'])

    return df, G

# ğŸ“ˆ ×¦×™×•×¨ ×’×¨×£ ×§×©×¨×™×
def visualize_network(G):
    logging.info("ğŸ“ ××¦×™×’ ×¨×©×ª ×§×©×¨×™×...")
    plt.figure(figsize=(10, 8))
    nx.draw(G, with_labels=True, node_size=700, node_color="lightblue", font_size=10)
    plt.title("Network Graph of Entities")
    path = os.path.join(EXPORT_DIR, "network_graph.png")
    plt.savefig(path)
    plt.close()
    logging.info(f"ğŸ–¼ ×’×¨×£ × ×©××¨: {path}")

# ğŸ“„ ×™×¦×™×¨×ª ×“×•×— PDF
def generate_report(df, filename="enriched_data_report.pdf"):
    from fpdf import FPDF
    pdf = FPDF()
    pdf.set_auto_page_break(auto=True, margin=15)
    pdf.add_page()
    pdf.set_font("Arial", size=12)
    pdf.cell(200, 10, txt='×“×•"×— × ×ª×•× ×™× ××¢×•×‘×“×™×', ln=True, align="C")
    pdf.ln(10)

    for _, row in df.iterrows():
        pdf.multi_cell(0, 10, txt=f"Entity ID: {row['entity_id']} | Related To: {row['related_entity_id']} | Cluster: {row['cluster']}")

    output_path = os.path.join(EXPORT_DIR, filename)
    pdf.output(output_path)
    logging.info(f"ğŸ“„ ×“×•×— PDF × ×©××¨: {output_path}")

# ğŸ§  ×¤×•× ×§×¦×™×™×ª ×¢×™×‘×•×“ ×¨××©×™×ª
def analyze_search_results():
    data = fetch_data_for_enrichment()
    if data:
        enriched = enrich_data(data)
        df, graph = perform_analysis_on_enriched_data(enriched)
        generate_report(df)
        visualize_network(graph)
        logging.info("âœ… ×¢×™×‘×•×“ ×”×¡×ª×™×™× ×‘×”×¦×œ×—×”.")
    else:
        logging.warning("âš ï¸ ××™×Ÿ × ×ª×•× ×™× ×œ×¢×™×‘×•×“.")

# ğŸ“Œ ×‘××™×“×” ×•××ª×” ××¨×™×¥ ×¢×¦×××™×ª ××ª ×”×§×•×‘×¥
if __name__ == "__main__":
    analyze_search_results()
